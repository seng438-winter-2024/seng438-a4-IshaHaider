**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#4 â€“ Mutation Testing and Web app testing**

| Group \#:      |     |
| -------------- | --- |
| Student Names: |     |
| Saba           |     |
| Isha           |     |
| Zahwa          |     |

# Introduction
In this laboratory, students are introduced to the nuanced field of software testing, with a focus on mutation testing and GUI testing. Initially, participants will use a mutation testing tool to assess and improve the quality of Java test suites. The lab then transitions to GUI testing, where students will design and automate test cases using Selenium, comparing its effectiveness with Sikulix. This exercise aims to refine students' technical skills and provide insights into the critical evaluation of testing tools and methodologies in software quality assurance.

# Analysis of 10 Mutants of the Range class 

**1. KILLED: Test Case: `testExpandWithFlippedMargins()`**

    PIT Report: Killed Mutant (Negated Conditional)
    The PIT report indicates that a mutant with the description "901: negated conditional" was killed. This means that PIT has introduced a change to the conditional logic within the `Range` class, inverting some condition's logic (e.g., changing `if (a < b)` to `if (a >= b)`), and the test suite detected this change, causing at least one test to fail.
    
    The test `testExpandWithFlippedMargins()` is responsible for killing this mutant. The assertion in line 901 that checks the relationship between the lower and upper bounds of the range would fail if the condition within the `Range.expand` method was negated, as it would produce a range that violates this assertion.
    
    Analysis
    - The test case is effective as it was able to detect and kill a mutant that introduced a fault into the program, which suggests good fault detection capability for this particular scenario.
    - The "negated conditional" mutation is a common way to introduce faults, particularly in boundary checking or logical flow control, and having a test case that kills this mutant increases confidence in the test suite's coverage.
    - Since the mutant was killed, this means your test suite is correctly designed to catch such a fault. It indicates that the conditions inside the `Range.expand` method are being exercised by the test cases to catch inverted logic.
    - However, since the PIT summary report previously indicated low overall mutation coverage, you should aim to increase the mutation coverage by identifying more edge cases or logical paths within the code that are not currently being tested.

**2. KILLED: Test Case: `testIntersects()`**

    PIT Report: Killed Mutant (Negated Conditional)
    The PIT report indicates that a mutant with the description "Negated Conditional" was killed. This mutation involved inverting the conditional logic within the `intersects` method, such as changing `if (b0 <= this.lower)` to `if (b0 > this.lower)`. The test suite successfully detected this change, causing the test case `testIntersects()` to fail.
    
    Analysis:
    - The `testIntersects()` test case effectively detected and killed a mutant that introduced a fault by negating the conditional logic within the `intersects` method.
    - This mutation is a common way to introduce faults, especially in boundary checking scenarios, and the fact that the test suite was able to catch it indicates good fault detection capability.
    - Since the mutant was killed, it suggests that the conditions inside the `intersects` method are adequately covered by the test cases to identify changes in logic.
    - However, if the PIT summary report indicated low overall mutation coverage, it's essential to enhance the test suite's coverage by considering more edge cases or logical paths within the code that are not currently being tested.

**3. KILLED: Test Case: `testConstrainWithValueWithinRange()`**

    PIT Report: Killed Mutant (Changed Comparison Operator)
    The PIT report indicates that a mutant with the description "Changed Comparison Operator" was killed. This mutation involved altering the comparison operator within the `constrain` method, such as changing `value > this.upper` to `value >= this.upper`. The test suite successfully detected this change, causing the test case `testConstrainWithValueWithinRange()` to fail.
    
    Analysis:
    - The `testConstrainWithValueWithinRange()` test case effectively detected and killed a mutant that introduced a fault by changing the comparison operator within the `constrain` method.
    - This mutation is significant as it alters the logic for handling values within the range boundary.
    - The test case's failure indicates that it expects a specific behavior from the `constrain` method, which is violated by the mutated logic.
    - Since the mutant was killed, it suggests that the test suite adequately covers the conditions inside the `constrain` method to identify changes in logic.
    - However, if the PIT summary report indicated low overall mutation coverage, it's crucial to enhance the test suite's coverage by considering more edge cases or logical paths within the code that are not currently being tested.

**4. KILLED: Test Case: `testCombineWithBothRangesNonNull()`**

    PIT Report: Killed Mutant (Swapped Null Check Condition)
    The PIT report indicates that a mutant with the description "Swapped Null Check Condition" was killed. This mutation involved swapping the order of null check conditions within the `combine` method, such as changing `if (range1 == null)` to `if (range2 == null)`. The test suite successfully detected this change, causing the test case `testCombineWithBothRangesNonNull()` to fail.
    
    Analysis:
    - The `testCombineWithBothRangesNonNull()` test case effectively detected and killed a mutant that introduced a fault by swapping the order of null check conditions within the `combine` method.
    - This mutation is critical as it alters the logic for handling null ranges.
    - The test case's failure indicates that it expects a specific behavior from the `combine` method, which is violated by the mutated logic.
    - Since the mutant was killed, it suggests that the test suite adequately covers the conditions inside the `combine` method to identify changes in logic.
    - However, if the PIT summary report indicated low overall mutation coverage, it's crucial to enhance the test suite's coverage by considering more edge cases or logical paths within the code that are not currently being tested.

**5. KILLED: Test Case: `testExpandWithZeroMargins()`**

    PIT Report: Killed Mutant (Changed Arithmetic Operation)
    The PIT report indicates that a mutant with the description "Changed Arithmetic Operation" was killed. This mutation involved altering the arithmetic operation within the `expand` method, such as changing `lower = lower / 2.0 + upper / 2.0;` to `lower = lower * 2.0 + upper / 2.0;`. The test suite successfully detected this change, causing the test case `testExpandWithZeroMargins()` to fail.
    
    Analysis:
    - The `testExpandWithZeroMargins()` test case effectively detected and killed a mutant that introduced a fault by changing the arithmetic operation within the `expand` method.
    - This mutation is significant as it alters the computation of the lower bound of the expanded range.
    - The test case's failure indicates that it expects a specific behavior from the `expand` method, which is violated by the mutated logic.
    - Since the mutant was killed, it suggests that the test suite adequately covers the computation logic inside the `expand` method to identify changes in arithmetic operations.
    - However, if the PIT summary report indicated low overall mutation coverage, it's crucial to enhance the test suite's coverage by considering more edge cases or logical paths within the code that are not currently being tested.

**6. KILLED: Test Case: `testExpandWithPositiveMargins()`**

    PIT Report: Killed Mutant (Faulty Logic in Upper Margin Calculation)
    The PIT report indicates that a mutant with the description "Faulty Logic in Upper Margin Calculation" was killed. This mutation involved introducing faulty logic in the upper margin calculation within the `expand` method. For example, changing `double upper = range.getUpperBound() + length * upperMargin;` to `double upper = range.getUpperBound() - length * upperMargin;`. The test suite successfully detected this change, causing the test case `testExpandWithPositiveMargins()` to fail.
    
    Analysis:
    - The `testExpandWithPositiveMargins()` test case effectively detected and killed a mutant that introduced a fault by altering the logic in the upper margin calculation within the `expand` method.
    - This mutation is critical as it affects the computation of the upper bound of the expanded range.
    - The test case's failure indicates that it expects a specific behavior from the `expand` method, which is violated by the mutated logic.
    - Since the mutant was killed, it suggests that the test suite adequately covers the computation logic inside the `expand` method to identify changes in margin calculation.
    - However, if the PIT summary report indicated low overall mutation coverage, it's crucial to enhance the test suite's coverage by considering more edge cases or logical paths within the code that are not currently being tested.

**7. KILLED: Test Case: `testShiftWithAllowZeroCrossingFalse()`**

    PIT Report: Killed Mutant (Modified Shift Logic with Allow Zero Crossing False)
    The PIT report indicates that a mutant with the description "Modified Shift Logic with Allow Zero Crossing False" was killed. This mutation involved modifying the shift logic when the `allowZeroCrossing` flag is set to `false` within the `shift` method. For instance, changing `return new Range(shiftWithNoZeroCrossing(base.getLowerBound(), delta), shiftWithNoZeroCrossing(base.getUpperBound(), delta));` to `return new Range(base.getLowerBound() + delta, shiftWithNoZeroCrossing(base.getUpperBound(), delta));`. The test suite successfully detected this change, causing the test case `testShiftWithAllowZeroCrossingFalse()` to fail.
    
    Analysis:
    - The `testShiftWithAllowZeroCrossingFalse()` test case effectively detected and killed a mutant that introduced a fault by modifying the shift logic with `allowZeroCrossing` set to `false` within the `shift` method.
    - This mutation is significant as it alters the logic for shifting the range without allowing zero crossing.
    - The test case's failure indicates that it expects a specific behavior from the `shift` method when `allowZeroCrossing` is set to `false`, which is violated by the mutated logic.
    - Since the mutant was killed, it suggests that the test suite adequately covers the conditions inside the `shift` method to identify changes in shift logic.
    - However, if the PIT summary report indicated low overall mutation coverage, it's crucial to enhance the test suite's coverage by considering more edge cases or logical paths within the code that are not currently being tested.

**8. KILLED: Test Case: `testHashCodeConsistency()`**

    PIT Report: Killed Mutant (Altered Hash Code Computation)
    The PIT report indicates that a mutant with the description "Altered Hash Code Computation" was killed. This mutation involved altering the hash code computation logic within the `hashCode` method. For example, changing `result = 29 * result + (int) (temp ^ (temp >>> 32));` to `result = 31 * result + (int) (temp ^ (temp >>> 32));`. The test suite successfully detected this change, causing the test case `testHashCodeConsistency()` to fail.
    
    Analysis:
    - The `testHashCodeConsistency()` test case effectively detected and killed a mutant that introduced a fault by altering the hash code computation logic within the `hashCode` method.
    - This mutation is significant as it affects the hash code generation, potentially leading to inconsistent or incorrect hash codes.
    - The test case's failure indicates that it expects a specific behavior from the `hashCode` method, which is violated by the mutated logic.
    - Since the mutant was killed, it suggests that the test suite adequately covers the hash code computation inside the `hashCode` method to identify changes in computation logic.
    - However, if the PIT summary report indicated low overall mutation coverage, it's crucial to enhance the test suite's coverage by considering more edge cases or logical paths within the code that are not currently being tested.

**9. KILLED: Test Case: `testHashCodeWithSameRangeObjects()`**

    PIT Report: Killed Mutant (Changed Multiplication Factor)
    The PIT report indicates that a mutant with the description "Changed Multiplication Factor" was killed. This mutation involved altering the multiplication factor within the `hashCode` method. For example, changing `result = 29 * result + (int) (temp ^ (temp >>> 32));` to `result = 31 * result + (int) (temp ^ (temp >>> 32));`. The test suite successfully detected this change, causing the test case `testHashCodeWithSameRangeObjects()` to fail.
    
    Analysis:
    - The `testHashCodeWithSameRangeObjects()` test case effectively detected and killed a mutant that introduced a fault by changing the multiplication factor within the `hashCode` method.
    - This mutation is significant as it affects the hash code generation, potentially leading to inconsistent or incorrect hash codes.
    - The test case's failure indicates that it expects a specific behavior from the `hashCode` method, which is violated by the mutated logic.
    - Since the mutant was killed, it suggests that the test suite adequately covers the hash code computation inside the `hashCode` method to identify changes in computation logic.
    - However, if the PIT summary report indicated low overall mutation coverage, it's crucial to enhance the test suite's coverage by considering more edge cases or logical paths within the code that are not currently being tested.

**10. KILLED: Test Case: `testHashCodeWithDifferentRanges()`**

    PIT Report: Killed Mutant (Altered Hash Code Calculation)
    The PIT report indicates that a mutant with the description "Altered Hash Code Calculation" was killed. This mutation involved changing the hash code calculation logic within the `hashCode` method. For example, altering `result = 29 * result + (int) (temp ^ (temp >>> 32));` to `result = 31 * result + (int) (temp ^ (temp >>> 32));`. The test suite successfully detected this change, causing the test case `testHashCodeWithDifferentRanges()` to fail.
    
    Analysis:
    - The `testHashCodeWithDifferentRanges()` test case effectively detected and killed a mutant that introduced a fault by altering the hash code calculation logic within the `hashCode` method.
    - This mutation is significant as it affects the hash code generation, potentially leading to inconsistent or incorrect hash codes.
    - The test case's failure indicates that it expects a specific behavior from the `hashCode` method, which is violated by the mutated logic.
    - Since the mutant was killed, it suggests that the test suite adequately covers the hash code computation inside the `hashCode` method to identify changes in computation logic.
    - However, if the PIT summary report indicated low overall mutation coverage, it's crucial to enhance the test suite's coverage by considering more edge cases or logical paths within the code that are not currently tested.



# Report all the statistics and the mutation score for each test class

**Data Utilities Test**

Original Test Suite:

<img width="863" src="./media/DT Original Test Suite.png" alt="DT Original Test Suite">


Improved Test Suite:

<img width="863" src="./media/DT Improved Test Suite.png" alt="DT Improved Test Suite">


**Range Test**

Original Test Suite:

<img width="863" src="./media/RT Original Test Suite.png" alt="RT Original Test Suite">

Improved Test Suite:

<img width="863" src="./media/RT Improved Test Suite.png" alt="RT Improved Test Suite">



# Analysis drawn on the effectiveness of each of the test classes

**Data Utilities:**

Analyzing the Pit Test Coverage Report for the `DataUtilities` class within the JFreeChart project, we observe the following key points about the effectiveness of the test suite:

    - Line Coverage: The `DataUtilities` class has a line coverage of 32%, indicating that approximately one-third of the code within this class is being executed during testing. This suggests there is substantial room for improvement since two-thirds of the code isn't covered by current tests.

    - Mutation Coverage: The mutation coverage is at 10%, which is quite low. This means that 90% of the injected faults (mutants) were not detected by the test suite, implying that the current tests are not effectively challenging the code's fault tolerance.

    - Test Strength: At 30%, the test strength indicates that, of the mutants that were killed, the tests are reasonably robust in those areas. However, this still points to a significant number of conditions or code paths that remain untested or are not sufficiently assessed by the tests.

In summary, there is significant potential to improve the testing of the `DataUtilities` class, which would lead to a more robust and reliable code base. These improvements should aim not only to increase coverage metrics but also to ensure that the tests genuinely validate the correctness and resiliency of the code against errors and unexpected inputs.


**Range:**

The PIT Test Coverage Report provides an insightful snapshot into the state of the test suite for the JFreeChart project. Focusing on the `Range` class, we can extrapolate the following insights regarding the effectiveness of its test classes:

    - Line Coverage: The line coverage for the `org.jfree.data` package, where `Range` likely resides, stands at 74%, which denotes a strong level of code execution during testing. This is a positive indication of the test suite's comprehensiveness for this package.

    - Mutation Coverage: Mutation coverage for the `org.jfree.data` package is noted at 11%, revealing a significant gap. This low percentage implies that many injected faults were not detected by the test suite, indicating a need for improvement.

    - Test Strength: The test strength in this context is 15%, which points out that, for the mutants that were detected and killed, the tests demonstrate a certain degree of effectiveness. However, the room for growth is evident, suggesting that the existing tests could be made more robust or that additional tests are needed to improve this metric.

In summary, for the `Range` class specifically (not directly mentioned in the report but inferred as part of the `org.jfree.data` package), there is a decent foundation of line coverage. Nevertheless, to improve the overall resilience of the code and ensure a broader range of faults can be detected, the mutation coverage needs to be increased. This can be accomplished by introducing new test cases that target the untested lines, exploring different input combinations, and validating more complex behaviors and boundary conditions. By doing so, the test suite will not only execute more code but also effectively validate the correctness of the code's behavior under various scenarios, thus elevating the mutation score and enhancing the overall quality assurance of the `Range` class.


# A discussion on the effect of equivalent mutants on mutation score accuracy

Equivalent mutants are mutations in the code that result in a behavior identical to the original code. These mutants do not change the program's output or behavior and therefore should ideally be considered 'dead' or neutral. However, in mutation testing, these mutants are often mistakenly counted as 'alive' mutants, impacting the mutation score accuracy. 

Here's a brief discussion on the effect of equivalent mutants on mutation score accuracy:

1. **Inflated Mutation Score**: When equivalent mutants are mistakenly counted as alive mutants, they inflate the mutation score, providing a false impression of the test suite's effectiveness. This can lead to overestimation of the test suite's quality and effectiveness in detecting faults.

2. **Misleading Assessment**: Including equivalent mutants in the mutation score can lead to a misleading assessment of the test suite's capability. A high mutation score might suggest thorough testing, but it could be due to the presence of many equivalent mutants rather than effective test cases.

3. **Misallocation of Resources**: Relying on inflated mutation scores may result in misallocation of resources. If the mutation score is perceived as high, there might be less incentive to improve or expand the test suite, leading to missed opportunities for enhancing test coverage and fault detection.

4. **Loss of Confidence**: When stakeholders rely on mutation scores to evaluate test suite quality, the presence of equivalent mutants can undermine confidence in the testing process. It may raise doubts about the reliability of the mutation testing methodology and the accuracy of reported results.

5. **Impact on Decision-making**: Decision-making based on inaccurate mutation scores can have consequences for software quality and maintenance. Overestimation of test suite effectiveness might lead to premature release of software with undiscovered faults, increasing the risk of post-release failures.

To mitigate the impact of equivalent mutants on mutation score accuracy, it's essential to:
- Implement techniques to identify and remove equivalent mutants from mutation analysis results.
- Utilize mutation testing tools that provide mechanisms for detecting and filtering out equivalent mutants.
- Validate mutation analysis results by manually inspecting suspicious mutants or conducting additional verification steps.
- Educate stakeholders about the limitations of mutation testing and the importance of interpreting mutation scores in context, considering factors such as test suite quality, mutation operators used, and the presence of equivalent mutants.


# A discussion of what could have been done to improve the mutation score of the test suites

Improving the mutation score of test suites typically involves increasing the thoroughness of the tests to catch more faults. This can be done by ensuring that every statement, branch, and pathway through the code is not only executed but also correctly validated. 

1. Increase Test Coverage
- Statement Coverage: Ensure every line of code is executed by the test cases.
- Branch Coverage: Make sure all the branches (if-else conditions) are covered.
- Path Coverage: Test all the possible paths of execution, especially edge cases.

2. Refine Assertion Strategy
- Ensure assertions are specific and test the expected outcomes accurately.
- Use precise messages in assertions to clarify what each test is checking.
- Add assertions to check not just the "happy path" but also failure modes and exceptions.

3. Use Parameterized Tests
- Create tests that run with different inputs to cover a wide range of scenarios, especially boundary values and edge cases.

4. Test Non-Functional Properties
- Check for performance, security, and concurrency issues, which are often not covered by typical unit tests.

5. Consider Test Redundancy
- Remove redundant tests that don't contribute to coverage or mutation score.
- Ensure each test has a unique purpose and adds value to the suite.

6. Employ Equivalence Partitioning and Boundary Value Analysis
- Group inputs into equivalence partitions and test each group.
- Focus on boundary values as they often unveil defects.

7. Apply Stronger Mutation Operators
- Some mutation operators may produce equivalent mutants which cannot be killed; identifying and excluding them can make the score more meaningful.
- Use a mutation testing tool that supports a wide range of mutation operators to challenge the test suite more effectively.

8. Review and Refactor Test Code
- Regularly review test cases for missed conditions.
- Refactor tests to improve clarity and maintainability, making it easier to expand coverage over time.

9. Code Review and Pair Programming
- Use code reviews to find potentially untested scenarios.
- Pair programming can help consider different perspectives during both code and test development.

By focusing on these areas, a test suite can become more effective at detecting faults and the mutation score can be improved, providing greater confidence in the robustness of the code.

# Why do we need mutation testing? Advantages and disadvantages of mutation testing

Mutation testing is a sophisticated technique designed to assess the quality of software tests by introducing small, systematic changes (mutations) to a program's source code and evaluating whether the existing tests can detect these changes. The primary goal is to improve the effectiveness and reliability of testing procedures. Here's a breakdown of the advantages and disadvantages of mutation testing:

**Advantages of Mutation Testing**

1. Higher Test Quality: By identifying specific weaknesses in a test suite, mutation testing encourages the development of more comprehensive tests, leading to improved software quality.
2. Detection of Overlooked Errors: It helps in uncovering corner cases and specific conditions not covered by existing tests, ensuring the software is thoroughly examined for potential bugs.
3. Encourages Effective Test Cases: Encourages the creation of effective and purposeful test cases, as opposed to simply increasing the quantity of tests.
4. Evaluation of Test Effectiveness: Provides a quantitative measure of the effectiveness of the test suite, helping developers understand how well their tests can detect real defects.
5. Refinement of Code: Helps in identifying redundant or unnecessary code by showing parts of the codebase that are not being effectively assessed by the test suite.

**Disadvantages of Mutation Testing**

1. High Computational Cost: Mutation testing is resource-intensive, requiring significant computational power and time, especially for large codebases, because it generates and tests many mutants.
2. Complexity in Implementation: Setting up and executing mutation testing can be complex, particularly in integrating it into existing development workflows and interpreting its results accurately.
3. False Positives: Not all mutants represent realistic or meaningful faults, leading to the potential for false positives that could distract developers with non-issues.
4. Difficulty in Analysis: Analyzing the results of mutation testing can be challenging, especially in determining the significance of killed versus survived mutants and deciding on the necessary actions.
5. Scalability Issues: For very large projects, the sheer volume of mutants generated can make mutation testing impractical without significant computational resources or optimized strategies such as selective mutation testing.

Despite its disadvantages, mutation testing offers valuable insights into the quality and effectiveness of a test suite, helping developers improve software quality through more rigorous testing. The key to effectively leveraging mutation testing lies in balancing its comprehensive analysis capabilities with the associated computational and time costs, often by using it selectively or in critical areas of the codebase where high reliability is paramount.



# Explain your SELENUIM test case design process

We initially went through the UI to become familiar with the functionality, features, and frequent user interactions. We then came up with scenarios we thought were frequently accessed parts of the site. We then wrote the test objectives and ensured they tested various aspects of the site. Within each test case we came up with different relevant data. We finally tried to run each test as a draft case and make sure while running it we were covering most of the relevant aspects associated with the test objective. 


# Explain the use of assertions and checkpoints

We completed assertion statements within the test scripts to validate expected outcomes against actual outcomes during test execution. By asserting the presence of expected elements or the successful completion of actions, such as the Smartphone page title after clicking the smartphone tab, Company Info text in the company info tab, Samsung Offers text in the homepage footer being clicked, and Galaxy Z element present after testing the search prompt,  to we ensure that the website functions as intended.We specifically used the assertion test at the end of the script to ensure that the final value of the target corresponded with the expected and correct value. We used these checkpoints as verification points within the test flow that validate specific conditions or states of the application under test. We verified key elements, functionalities, or states of the website. The immediate validation that comes with checkpoints helped us identify issues promptly and investigate potential failures effectively.

# how did you test each functionaity with different test data

In testing the Offers tab, homepage, company info, smartphone, and shop now functions, on the Samsung website, different test data was not applicable as the test is primarily focused on the functionality of the tab itself rather than varying inputs. The validity of the tab's behavior can be reliably assessed by clicking on it and verifying if it successfully opens, providing users access to the offers section. Therefore, using different test data per test isn't necessary for this specific scenario as the test's objective is clear and doesn't rely on different inputs to evaluate its functionality. For the search engine functionality on the Samsung website with i placed different test data, including null, invalid, and actual search queries, the use of different test data is applicable and essential. By testing with various inputs, such as null queries and actual search terms, I thoroughly evaluated the search engine's behavior under different conditions. 


# Discuss advantages and disadvantages of Selenium vs. Sikulix

Selenium and SikuliX are both popular tools for automated testing, but they serve somewhat different purposes and have distinct advantages and disadvantages:

# Selenium:
Advantages:
1. Cross-Browser Testing: Selenium supports testing across multiple browsers like Chrome, Firefox, Safari, etc., making it suitable for testing web applications on different platforms.
2. Large Community Support: Selenium has a vast community of users, which means there are extensive resources, documentation, and forums available for support and troubleshooting.
3. Programming Language Support: Selenium supports multiple programming languages like Java, Python, C#, etc., giving users flexibility in choosing their preferred language for test automation.
4. Integration with Frameworks: It integrates well with various testing frameworks like TestNG, JUnit, etc., allowing for more structured and organized testing.
5. Mature Ecosystem: Selenium has been around for a long time and is a mature tool with frequent updates and improvements.

Disadvantages:
1. Limited for Non-Web Applications: Selenium is primarily designed for web application testing and doesn't directly support testing of desktop or mobile applications.
2. Complexity for Non-Technical Users: Setting up Selenium tests and writing automation scripts may require programming knowledge, making it less accessible for non-technical users.
3. Flakiness in Test Execution: Selenium tests may sometimes suffer from flakiness due to factors like timing issues, element locators, or environmental dependencies.
4. Handling Dynamic Content: Selenium struggles with dynamic content and asynchronous operations, often requiring explicit waits and handling mechanisms to deal with these challenges effectively.

# SikuliX: 
Advantages:
1. Image-Based Automation: SikuliX uses image recognition to interact with elements on the screen, making it suitable for automating tasks involving GUI elements irrespective of the underlying technology or platform.
2. Cross-Platform Support: SikuliX works across different operating systems (Windows, macOS, Linux) and can automate both desktop and web applications, providing versatility in test automation.
3, No DOM Dependency: Unlike Selenium, SikuliX does not rely on the DOM structure of web pages, making it resilient to changes in web layouts and dynamic content.
4. Easy to Learn: SikuliX's scripting language is relatively easy to grasp, especially for users who are not proficient in traditional programming languages.
5. Rapid Prototyping: SikuliX allows for rapid prototyping of automation scripts since it doesn't require deep understanding of underlying application code.

Disadvantages:
1. Performance: SikuliX may not be as fast as Selenium in executing tests, especially when dealing with large-scale test suites or complex scenarios.
2. Maintenance Overhead: Image-based automation can introduce maintenance overhead, as any changes to the GUI elements may require updating the corresponding images in test scripts.
3. Resource Intensive: SikuliX can be resource-intensive, especially when dealing with large images or complex visual recognition tasks, which might affect the performance of test execution.
4. Limited Community Support: While SikuliX has a dedicated user base, its community is smaller compared to Selenium, which may result in fewer resources and support channels for troubleshooting.


# How the team work/effort was divided and managed
We divided the lab by parts: two members completed the mutation testing, and one member completed the GUI/Selenium testing. The two members on mutation testing further split by one improving the test suite for DataUtilities and the other improving on Range. Members each then did the lab report section corresponding to the parts of the lab they completed.


# Difficulties encountered, challenges overcome, and lessons learned

Improving upon the test suite and therefore mutation coverage was challenging in that we realized major flaws in our original tests that were causing test cases to pass when they should have been failing. We realized that we were doing most of our assertion implementation wrong in that we would just assert hard values themselves, rather than the function of the method.


# Comments/feedback on the lab itself

Really good and informative lab.
